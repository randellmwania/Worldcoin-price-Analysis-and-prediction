{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>200.230327</td>\n",
       "      <td>212.106582</td>\n",
       "      <td>198.374500</td>\n",
       "      <td>205.695640</td>\n",
       "      <td>7.132699e+10</td>\n",
       "      <td>9.150957e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>2024-09-19</td>\n",
       "      <td>188.305880</td>\n",
       "      <td>199.477624</td>\n",
       "      <td>184.037433</td>\n",
       "      <td>199.477624</td>\n",
       "      <td>4.713701e+10</td>\n",
       "      <td>8.460954e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>182.005667</td>\n",
       "      <td>189.902632</td>\n",
       "      <td>179.929701</td>\n",
       "      <td>187.779677</td>\n",
       "      <td>4.191690e+10</td>\n",
       "      <td>8.099784e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>189.331293</td>\n",
       "      <td>189.331293</td>\n",
       "      <td>178.401239</td>\n",
       "      <td>182.249198</td>\n",
       "      <td>5.452660e+10</td>\n",
       "      <td>7.791811e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-09-15</td>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>194.384816</td>\n",
       "      <td>201.122540</td>\n",
       "      <td>188.887849</td>\n",
       "      <td>189.929580</td>\n",
       "      <td>4.570792e+10</td>\n",
       "      <td>8.292824e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>266.303801</td>\n",
       "      <td>286.533403</td>\n",
       "      <td>264.554328</td>\n",
       "      <td>285.644185</td>\n",
       "      <td>6.007178e+10</td>\n",
       "      <td>2.919856e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>287.676920</td>\n",
       "      <td>295.834139</td>\n",
       "      <td>268.693036</td>\n",
       "      <td>269.989743</td>\n",
       "      <td>9.477407e+10</td>\n",
       "      <td>2.970630e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>293.852710</td>\n",
       "      <td>321.444574</td>\n",
       "      <td>282.445740</td>\n",
       "      <td>289.173453</td>\n",
       "      <td>1.894057e+11</td>\n",
       "      <td>3.163373e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>278.619394</td>\n",
       "      <td>309.644293</td>\n",
       "      <td>245.956788</td>\n",
       "      <td>293.075495</td>\n",
       "      <td>1.737368e+11</td>\n",
       "      <td>2.890207e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>232.927358</td>\n",
       "      <td>414.103163</td>\n",
       "      <td>227.284983</td>\n",
       "      <td>283.325462</td>\n",
       "      <td>6.015643e+10</td>\n",
       "      <td>2.903896e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Start         End        Open        High         Low       Close  \\\n",
       "0    2024-09-19  2024-09-20  200.230327  212.106582  198.374500  205.695640   \n",
       "1    2024-09-18  2024-09-19  188.305880  199.477624  184.037433  199.477624   \n",
       "2    2024-09-17  2024-09-18  182.005667  189.902632  179.929701  187.779677   \n",
       "3    2024-09-16  2024-09-17  189.331293  189.331293  178.401239  182.249198   \n",
       "4    2024-09-15  2024-09-16  194.384816  201.122540  188.887849  189.929580   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "419  2023-07-28  2023-07-29  266.303801  286.533403  264.554328  285.644185   \n",
       "420  2023-07-27  2023-07-28  287.676920  295.834139  268.693036  269.989743   \n",
       "421  2023-07-26  2023-07-27  293.852710  321.444574  282.445740  289.173453   \n",
       "422  2023-07-25  2023-07-26  278.619394  309.644293  245.956788  293.075495   \n",
       "423  2023-07-24  2023-07-25  232.927358  414.103163  227.284983  283.325462   \n",
       "\n",
       "           Volume    Market Cap  \n",
       "0    7.132699e+10  9.150957e+10  \n",
       "1    4.713701e+10  8.460954e+10  \n",
       "2    4.191690e+10  8.099784e+10  \n",
       "3    5.452660e+10  7.791811e+10  \n",
       "4    4.570792e+10  8.292824e+10  \n",
       "..            ...           ...  \n",
       "419  6.007178e+10  2.919856e+10  \n",
       "420  9.477407e+10  2.970630e+10  \n",
       "421  1.894057e+11  3.163373e+10  \n",
       "422  1.737368e+11  2.890207e+10  \n",
       "423  6.015643e+10  2.903896e+10  \n",
       "\n",
       "[424 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data reading\n",
    "df = pd.read_csv(\"worldcoin-org_2023-07-24_2024-09-20.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Start and End columns to datetime\n",
    "df[\"Start\"] = pd.to_datetime(df[\"Start\"])\n",
    "df[\"End\"] = pd.to_datetime(df[\"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features and target variable\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\", \"Market Cap\"]]\n",
    "y = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 5s - loss: 219277.2500\n",
      "Epoch 1: val_loss improved from inf to 279211.28125, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 48ms/step - loss: 260274.5312 - val_loss: 279211.2812 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 263345.1875\n",
      "Epoch 2: val_loss improved from 279211.28125 to 278417.75000, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 259868.4844 - val_loss: 278417.7500 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 324346.5312\n",
      "Epoch 3: val_loss improved from 278417.75000 to 276276.06250, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 258771.6875 - val_loss: 276276.0625 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 256867.2656\n",
      "Epoch 4: val_loss improved from 276276.06250 to 270268.68750, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 255369.4688 - val_loss: 270268.6875 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 322004.0625\n",
      "Epoch 5: val_loss improved from 270268.68750 to 253863.76562, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 246607.6562 - val_loss: 253863.7656 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 252829.3750\n",
      "Epoch 6: val_loss improved from 253863.76562 to 214984.81250, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 223409.6406 - val_loss: 214984.8125 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 110501.7656\n",
      "Epoch 7: val_loss improved from 214984.81250 to 138898.73438, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 173137.4688 - val_loss: 138898.7344 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 112148.6719\n",
      "Epoch 8: val_loss improved from 138898.73438 to 39446.85938, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 92915.2734 - val_loss: 39446.8594 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 41324.8242\n",
      "Epoch 9: val_loss improved from 39446.85938 to 10946.64746, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 19675.5391 - val_loss: 10946.6475 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 21103.9219\n",
      "Epoch 10: val_loss improved from 10946.64746 to 8564.99316, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 21524.8438 - val_loss: 8564.9932 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 18067.8984\n",
      "Epoch 11: val_loss did not improve from 8564.99316\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 12089.3047 - val_loss: 8744.7197 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6308.9009\n",
      "Epoch 12: val_loss improved from 8564.99316 to 8534.04297, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 11131.5791 - val_loss: 8534.0430 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8954.6680\n",
      "Epoch 13: val_loss improved from 8534.04297 to 4842.69482, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 8944.6572 - val_loss: 4842.6948 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6432.2832\n",
      "Epoch 14: val_loss improved from 4842.69482 to 4346.89746, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 8261.6914 - val_loss: 4346.8975 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6857.1924\n",
      "Epoch 15: val_loss improved from 4346.89746 to 3994.11353, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 10373.7969 - val_loss: 3994.1135 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4057.3115\n",
      "Epoch 16: val_loss did not improve from 3994.11353\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7904.9639 - val_loss: 4112.5820 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6257.5112\n",
      "Epoch 17: val_loss improved from 3994.11353 to 3487.59546, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 9927.8057 - val_loss: 3487.5955 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 13373.7949\n",
      "Epoch 18: val_loss improved from 3487.59546 to 2997.39673, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 8266.9854 - val_loss: 2997.3967 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7372.9141\n",
      "Epoch 19: val_loss improved from 2997.39673 to 2792.31665, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 8544.9424 - val_loss: 2792.3167 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5984.1318\n",
      "Epoch 20: val_loss improved from 2792.31665 to 2516.02344, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6722.7686 - val_loss: 2516.0234 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6551.1489\n",
      "Epoch 21: val_loss did not improve from 2516.02344\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8219.7451 - val_loss: 2994.6570 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3830.3943\n",
      "Epoch 22: val_loss did not improve from 2516.02344\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8105.6880 - val_loss: 2900.2153 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5171.7642\n",
      "Epoch 23: val_loss improved from 2516.02344 to 2236.75464, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 8432.0752 - val_loss: 2236.7546 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9240.5693\n",
      "Epoch 24: val_loss did not improve from 2236.75464\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8019.4824 - val_loss: 2383.1580 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4196.9370\n",
      "Epoch 25: val_loss improved from 2236.75464 to 2190.35522, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 7268.0835 - val_loss: 2190.3552 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6665.9854\n",
      "Epoch 26: val_loss improved from 2190.35522 to 2165.99731, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7770.0454 - val_loss: 2165.9973 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6088.3096\n",
      "Epoch 27: val_loss did not improve from 2165.99731\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5416.7773 - val_loss: 2235.2576 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5037.7583\n",
      "Epoch 28: val_loss improved from 2165.99731 to 2014.67114, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7156.1636 - val_loss: 2014.6711 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "5/9 [===============>..............] - ETA: 0s - loss: 5251.5127\n",
      "Epoch 29: val_loss improved from 2014.67114 to 1935.88330, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 5849.9688 - val_loss: 1935.8833 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7484.5176\n",
      "Epoch 30: val_loss did not improve from 1935.88330\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5664.6821 - val_loss: 2042.9692 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8692.2852\n",
      "Epoch 31: val_loss did not improve from 1935.88330\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6478.4409 - val_loss: 2318.6147 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6854.3540\n",
      "Epoch 32: val_loss improved from 1935.88330 to 1835.77295, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 6915.7114 - val_loss: 1835.7729 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10105.7930\n",
      "Epoch 33: val_loss did not improve from 1835.77295\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7374.4971 - val_loss: 1870.4545 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8776.9590\n",
      "Epoch 34: val_loss did not improve from 1835.77295\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7232.5332 - val_loss: 2181.7639 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5229.3574\n",
      "Epoch 35: val_loss improved from 1835.77295 to 1731.77185, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6937.7129 - val_loss: 1731.7719 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10006.2246\n",
      "Epoch 36: val_loss did not improve from 1731.77185\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6881.8179 - val_loss: 1761.2568 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6304.1729\n",
      "Epoch 37: val_loss did not improve from 1731.77185\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 6487.7603 - val_loss: 2234.2817 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4979.6743\n",
      "Epoch 38: val_loss improved from 1731.77185 to 1612.05566, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6572.0293 - val_loss: 1612.0557 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7078.2734\n",
      "Epoch 39: val_loss improved from 1612.05566 to 1554.26538, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5872.8726 - val_loss: 1554.2654 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3295.9390\n",
      "Epoch 40: val_loss did not improve from 1554.26538\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6331.7456 - val_loss: 1844.2811 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4933.8374\n",
      "Epoch 41: val_loss did not improve from 1554.26538\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6539.2036 - val_loss: 1996.4786 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5425.8599\n",
      "Epoch 42: val_loss improved from 1554.26538 to 1506.50720, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6515.5078 - val_loss: 1506.5072 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5791.7793\n",
      "Epoch 43: val_loss did not improve from 1506.50720\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6327.2217 - val_loss: 1661.2375 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6139.5215\n",
      "Epoch 44: val_loss did not improve from 1506.50720\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 4673.9253 - val_loss: 1723.6382 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6762.2178\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1506.50720\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6692.0996 - val_loss: 1559.1764 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6393.4087\n",
      "Epoch 46: val_loss improved from 1506.50720 to 1459.01648, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 4772.8770 - val_loss: 1459.0165 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4527.9771\n",
      "Epoch 47: val_loss improved from 1459.01648 to 1416.91528, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7544.3286 - val_loss: 1416.9153 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5991.4023\n",
      "Epoch 48: val_loss improved from 1416.91528 to 1353.69849, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6464.6855 - val_loss: 1353.6985 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4342.0073\n",
      "Epoch 49: val_loss improved from 1353.69849 to 1352.71118, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 5550.5591 - val_loss: 1352.7112 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 17072.5391\n",
      "Epoch 50: val_loss did not improve from 1352.71118\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8698.6904 - val_loss: 1357.9736 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3946.3086\n",
      "Epoch 51: val_loss improved from 1352.71118 to 1337.89319, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6212.5127 - val_loss: 1337.8932 - lr: 5.0000e-04\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4137.5376\n",
      "Epoch 52: val_loss did not improve from 1337.89319\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 4600.7217 - val_loss: 1366.6801 - lr: 5.0000e-04\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5013.2485\n",
      "Epoch 53: val_loss improved from 1337.89319 to 1302.70032, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 6090.7412 - val_loss: 1302.7003 - lr: 5.0000e-04\n",
      "Epoch 54/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12846.7871\n",
      "Epoch 54: val_loss improved from 1302.70032 to 1283.87280, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5961.6294 - val_loss: 1283.8728 - lr: 5.0000e-04\n",
      "Epoch 55/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6818.2949\n",
      "Epoch 55: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6424.4966 - val_loss: 1556.3525 - lr: 5.0000e-04\n",
      "Epoch 56/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6531.0547\n",
      "Epoch 56: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6106.0825 - val_loss: 1970.8258 - lr: 5.0000e-04\n",
      "Epoch 57/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3182.1638\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 57: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5279.1611 - val_loss: 1477.3335 - lr: 5.0000e-04\n",
      "Epoch 58/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6720.9951\n",
      "Epoch 58: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6953.4717 - val_loss: 1495.1417 - lr: 2.5000e-04\n",
      "Epoch 59/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5187.9038\n",
      "Epoch 59: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5541.2842 - val_loss: 1452.5049 - lr: 2.5000e-04\n",
      "Epoch 60/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3341.4268\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5862.4312 - val_loss: 1495.6440 - lr: 2.5000e-04\n",
      "Epoch 61/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2404.4854\n",
      "Epoch 61: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 7168.9736 - val_loss: 1509.6925 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8477.1504\n",
      "Epoch 62: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5440.6206 - val_loss: 1441.6771 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4132.3496\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 63: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5276.6450 - val_loss: 1447.7238 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9405.0127Restoring model weights from the end of the best epoch: 54.\n",
      "\n",
      "Epoch 64: val_loss did not improve from 1283.87280\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 4921.2529 - val_loss: 1496.5530 - lr: 6.2500e-05\n",
      "Epoch 64: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Increase Model Complexity\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, \n",
    "                    validation_split=0.2, verbose=1, \n",
    "                    callbacks=[lr_scheduler, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 5s - loss: 156744.3750\n",
      "Epoch 1: val_loss improved from inf to 305506.37500, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 41ms/step - loss: 253461.8750 - val_loss: 305506.3750 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 191437.1875\n",
      "Epoch 2: val_loss improved from 305506.37500 to 303736.65625, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 252648.1250 - val_loss: 303736.6562 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 203830.9219\n",
      "Epoch 3: val_loss improved from 303736.65625 to 298443.34375, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 250200.7031 - val_loss: 298443.3438 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 191548.0938\n",
      "Epoch 4: val_loss improved from 298443.34375 to 283841.93750, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 243287.5156 - val_loss: 283841.9375 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 147616.1250\n",
      "Epoch 5: val_loss improved from 283841.93750 to 248983.79688, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 223889.5625 - val_loss: 248983.7969 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 203105.1406\n",
      "Epoch 6: val_loss improved from 248983.79688 to 177395.73438, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 183051.5938 - val_loss: 177395.7344 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 123111.1875\n",
      "Epoch 7: val_loss improved from 177395.73438 to 66575.32031, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 113466.4531 - val_loss: 66575.3203 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 53871.7656\n",
      "Epoch 8: val_loss improved from 66575.32031 to 6607.64990, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 28587.0234 - val_loss: 6607.6499 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7716.1885\n",
      "Epoch 9: val_loss did not improve from 6607.64990\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 18296.9355 - val_loss: 9368.3398 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 17211.3359\n",
      "Epoch 10: val_loss improved from 6607.64990 to 5830.41602, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 11956.3535 - val_loss: 5830.4160 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 13837.3398\n",
      "Epoch 11: val_loss did not improve from 5830.41602\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 10848.9395 - val_loss: 6850.4961 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9224.5986\n",
      "Epoch 12: val_loss improved from 5830.41602 to 4130.42725, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 10077.0293 - val_loss: 4130.4272 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9063.6260\n",
      "Epoch 13: val_loss improved from 4130.42725 to 3519.45874, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 9264.0234 - val_loss: 3519.4587 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 15698.6523\n",
      "Epoch 14: val_loss did not improve from 3519.45874\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 9678.0459 - val_loss: 3888.4512 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 20192.9883\n",
      "Epoch 15: val_loss did not improve from 3519.45874\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 9210.8096 - val_loss: 3524.6306 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7040.1631\n",
      "Epoch 16: val_loss improved from 3519.45874 to 2814.68091, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7175.7148 - val_loss: 2814.6809 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5347.6782\n",
      "Epoch 17: val_loss improved from 2814.68091 to 2612.37842, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6297.5933 - val_loss: 2612.3784 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5824.8628\n",
      "Epoch 18: val_loss did not improve from 2612.37842\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 7676.3589 - val_loss: 2925.7532 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6289.7319\n",
      "Epoch 19: val_loss improved from 2612.37842 to 2331.65552, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7885.4878 - val_loss: 2331.6555 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3625.6614\n",
      "Epoch 20: val_loss improved from 2331.65552 to 2283.80249, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7936.1772 - val_loss: 2283.8025 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6279.2832\n",
      "Epoch 21: val_loss improved from 2283.80249 to 2161.17554, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5878.8457 - val_loss: 2161.1755 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3763.0354\n",
      "Epoch 22: val_loss improved from 2161.17554 to 2102.22803, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 7410.0093 - val_loss: 2102.2280 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4580.0225\n",
      "Epoch 23: val_loss did not improve from 2102.22803\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6309.8677 - val_loss: 2432.6084 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5678.8521\n",
      "Epoch 24: val_loss improved from 2102.22803 to 2070.75342, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 6879.3213 - val_loss: 2070.7534 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12526.5742\n",
      "Epoch 25: val_loss did not improve from 2070.75342\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7350.7505 - val_loss: 2172.4065 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 1993.7146\n",
      "Epoch 26: val_loss improved from 2070.75342 to 2002.45471, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6046.3989 - val_loss: 2002.4547 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5419.8555\n",
      "Epoch 27: val_loss did not improve from 2002.45471\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7131.7783 - val_loss: 2243.8071 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7515.1631\n",
      "Epoch 28: val_loss improved from 2002.45471 to 1927.82446, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5622.9253 - val_loss: 1927.8245 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4850.2979\n",
      "Epoch 29: val_loss improved from 1927.82446 to 1890.70984, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6009.2124 - val_loss: 1890.7098 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3945.5376\n",
      "Epoch 30: val_loss did not improve from 1890.70984\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5127.8696 - val_loss: 2545.2378 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9297.2617\n",
      "Epoch 31: val_loss did not improve from 1890.70984\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6862.1348 - val_loss: 2011.4283 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8902.4492\n",
      "Epoch 32: val_loss improved from 1890.70984 to 1861.95959, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 6827.1206 - val_loss: 1861.9596 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4323.3140\n",
      "Epoch 33: val_loss did not improve from 1861.95959\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5504.5386 - val_loss: 2099.9065 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5393.0366\n",
      "Epoch 34: val_loss improved from 1861.95959 to 1790.20984, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5781.7471 - val_loss: 1790.2098 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4978.1860\n",
      "Epoch 35: val_loss did not improve from 1790.20984\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6410.8164 - val_loss: 1843.7769 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4347.2012\n",
      "Epoch 36: val_loss did not improve from 1790.20984\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6889.6592 - val_loss: 1927.0322 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6634.4883\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1790.20984\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6630.9419 - val_loss: 2124.4312 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8113.2930\n",
      "Epoch 38: val_loss did not improve from 1790.20984\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 6017.6445 - val_loss: 1902.8585 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9990.2988\n",
      "Epoch 39: val_loss improved from 1790.20984 to 1697.77991, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5747.7026 - val_loss: 1697.7799 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5449.4048\n",
      "Epoch 40: val_loss improved from 1697.77991 to 1686.67175, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5643.0337 - val_loss: 1686.6718 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6262.5537\n",
      "Epoch 41: val_loss did not improve from 1686.67175\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5601.9580 - val_loss: 1959.3672 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3550.2607\n",
      "Epoch 42: val_loss did not improve from 1686.67175\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 4675.1221 - val_loss: 1926.1947 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9338.1660\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1686.67175\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6168.9731 - val_loss: 2175.5081 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5390.3115\n",
      "Epoch 44: val_loss did not improve from 1686.67175\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5729.0127 - val_loss: 2032.8927 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4937.1753\n",
      "Epoch 45: val_loss did not improve from 1686.67175\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6254.0562 - val_loss: 1735.7939 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5084.6064\n",
      "Epoch 46: val_loss improved from 1686.67175 to 1684.85486, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 5974.1948 - val_loss: 1684.8549 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4305.0122\n",
      "Epoch 47: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6286.7310 - val_loss: 1984.7560 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6799.7441\n",
      "Epoch 48: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 7009.2197 - val_loss: 1999.5419 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2636.7126\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 5337.3247 - val_loss: 1851.5197 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4256.6719\n",
      "Epoch 50: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 4337.6035 - val_loss: 1758.9249 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6560.0854\n",
      "Epoch 51: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5277.6011 - val_loss: 1763.5636 - lr: 1.2500e-04\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5511.1055\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5544.8188 - val_loss: 1790.6621 - lr: 1.2500e-04\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10145.9824\n",
      "Epoch 53: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5922.3970 - val_loss: 1832.3925 - lr: 6.2500e-05\n",
      "Epoch 54/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 15045.1084\n",
      "Epoch 54: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6115.2344 - val_loss: 1851.4644 - lr: 6.2500e-05\n",
      "Epoch 55/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4833.0703\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 55: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 4447.7607 - val_loss: 1881.7792 - lr: 6.2500e-05\n",
      "Epoch 56/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4130.5786Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1684.85486\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5660.4014 - val_loss: 1946.6023 - lr: 3.1250e-05\n",
      "Epoch 56: early stopping\n",
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 6s - loss: 232060.6250\n",
      "Epoch 1: val_loss improved from inf to 265640.28125, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 41ms/step - loss: 263459.7188 - val_loss: 265640.2812 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 298816.0000\n",
      "Epoch 2: val_loss improved from 265640.28125 to 264171.50000, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 262485.0938 - val_loss: 264171.5000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 248024.2500\n",
      "Epoch 3: val_loss improved from 264171.50000 to 260052.12500, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 260008.3594 - val_loss: 260052.1250 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 419503.0000\n",
      "Epoch 4: val_loss improved from 260052.12500 to 248361.29688, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 253809.0625 - val_loss: 248361.2969 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 408618.9375\n",
      "Epoch 5: val_loss improved from 248361.29688 to 219140.29688, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 236453.3438 - val_loss: 219140.2969 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 194371.7344\n",
      "Epoch 6: val_loss improved from 219140.29688 to 155604.79688, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 191589.2656 - val_loss: 155604.7969 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 93968.6172\n",
      "Epoch 7: val_loss improved from 155604.79688 to 55650.16016, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 108086.7891 - val_loss: 55650.1602 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 51336.2812\n",
      "Epoch 8: val_loss improved from 55650.16016 to 6403.56641, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 26465.4355 - val_loss: 6403.5664 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 15301.4316\n",
      "Epoch 9: val_loss did not improve from 6403.56641\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 19023.1738 - val_loss: 7151.9526 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 16574.3672\n",
      "Epoch 10: val_loss improved from 6403.56641 to 6216.22754, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 12412.7168 - val_loss: 6216.2275 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10147.0156\n",
      "Epoch 11: val_loss improved from 6216.22754 to 5972.44971, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 10437.4355 - val_loss: 5972.4497 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9499.3877\n",
      "Epoch 12: val_loss improved from 5972.44971 to 3515.03442, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 9172.3633 - val_loss: 3515.0344 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6986.7397\n",
      "Epoch 13: val_loss improved from 3515.03442 to 2938.61816, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 8319.5889 - val_loss: 2938.6182 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6062.4424\n",
      "Epoch 14: val_loss did not improve from 2938.61816\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 7083.4482 - val_loss: 3311.6982 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10457.9414\n",
      "Epoch 15: val_loss did not improve from 2938.61816\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 7675.2539 - val_loss: 3681.0542 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11256.0508\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 2938.61816\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7653.6611 - val_loss: 2938.8530 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4688.6055\n",
      "Epoch 17: val_loss improved from 2938.61816 to 2574.79370, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 7791.0527 - val_loss: 2574.7937 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4345.2642\n",
      "Epoch 18: val_loss did not improve from 2574.79370\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8572.4277 - val_loss: 2874.4094 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7647.9243\n",
      "Epoch 19: val_loss did not improve from 2574.79370\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7677.2305 - val_loss: 2862.9863 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9133.4629\n",
      "Epoch 20: val_loss improved from 2574.79370 to 2522.56079, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 10297.7275 - val_loss: 2522.5608 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3126.0200\n",
      "Epoch 21: val_loss improved from 2522.56079 to 2354.01489, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6235.6538 - val_loss: 2354.0149 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8243.7090\n",
      "Epoch 22: val_loss did not improve from 2354.01489\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6822.5098 - val_loss: 2432.0349 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8043.6060\n",
      "Epoch 23: val_loss improved from 2354.01489 to 2277.07617, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 5959.6069 - val_loss: 2277.0762 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8745.8311\n",
      "Epoch 24: val_loss did not improve from 2277.07617\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7548.0601 - val_loss: 2341.3420 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9884.2227\n",
      "Epoch 25: val_loss improved from 2277.07617 to 2274.82666, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7815.8457 - val_loss: 2274.8267 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4550.7070\n",
      "Epoch 26: val_loss improved from 2274.82666 to 2250.69775, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6507.2261 - val_loss: 2250.6978 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4803.9478\n",
      "Epoch 27: val_loss did not improve from 2250.69775\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7240.1948 - val_loss: 2774.7183 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6080.0127\n",
      "Epoch 28: val_loss did not improve from 2250.69775\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6138.2852 - val_loss: 2788.0139 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4609.2070\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 2250.69775\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7188.3608 - val_loss: 2336.3916 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5288.4307\n",
      "Epoch 30: val_loss did not improve from 2250.69775\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6102.9932 - val_loss: 2321.6829 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8464.4639\n",
      "Epoch 31: val_loss improved from 2250.69775 to 2191.23926, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 6207.9204 - val_loss: 2191.2393 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6623.4111\n",
      "Epoch 32: val_loss improved from 2191.23926 to 2175.59961, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 4850.2339 - val_loss: 2175.5996 - lr: 2.5000e-04\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6733.1450\n",
      "Epoch 33: val_loss improved from 2175.59961 to 2054.01782, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7554.2505 - val_loss: 2054.0178 - lr: 2.5000e-04\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9337.1631\n",
      "Epoch 34: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7570.2456 - val_loss: 2326.7393 - lr: 2.5000e-04\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 13977.2725\n",
      "Epoch 35: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6867.7720 - val_loss: 2573.3254 - lr: 2.5000e-04\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4039.2393\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 36: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5517.3320 - val_loss: 2134.9685 - lr: 2.5000e-04\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4506.4072\n",
      "Epoch 37: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5901.8174 - val_loss: 2114.8298 - lr: 1.2500e-04\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3672.6958\n",
      "Epoch 38: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6302.5610 - val_loss: 2274.3552 - lr: 1.2500e-04\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3309.0093\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 39: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6999.2700 - val_loss: 2260.3987 - lr: 1.2500e-04\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4564.8696\n",
      "Epoch 40: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6606.2896 - val_loss: 2291.6887 - lr: 6.2500e-05\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12451.8438\n",
      "Epoch 41: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7851.0229 - val_loss: 2376.8809 - lr: 6.2500e-05\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6024.4424\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 42: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5962.8716 - val_loss: 2496.8145 - lr: 6.2500e-05\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8398.5098Restoring model weights from the end of the best epoch: 33.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 2054.01782\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6404.1216 - val_loss: 2614.0254 - lr: 3.1250e-05\n",
      "Epoch 43: early stopping\n",
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 5s - loss: 295333.7500\n",
      "Epoch 1: val_loss improved from inf to 232693.85938, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 271788.1875 - val_loss: 232693.8594 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 249953.9688\n",
      "Epoch 2: val_loss improved from 232693.85938 to 231339.04688, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 270903.4375 - val_loss: 231339.0469 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 330786.9375\n",
      "Epoch 3: val_loss improved from 231339.04688 to 227522.17188, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 268470.0000 - val_loss: 227522.1719 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 406317.5625\n",
      "Epoch 4: val_loss improved from 227522.17188 to 217299.50000, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 261674.1875 - val_loss: 217299.5000 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 246232.2188\n",
      "Epoch 5: val_loss improved from 217299.50000 to 191623.00000, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 242245.9062 - val_loss: 191623.0000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 243463.6719\n",
      "Epoch 6: val_loss improved from 191623.00000 to 136751.82812, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 203477.5312 - val_loss: 136751.8281 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 201820.6250\n",
      "Epoch 7: val_loss improved from 136751.82812 to 51369.81641, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 126055.4062 - val_loss: 51369.8164 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 33936.7031\n",
      "Epoch 8: val_loss improved from 51369.81641 to 6210.21729, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 31298.1777 - val_loss: 6210.2173 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7241.9409\n",
      "Epoch 9: val_loss did not improve from 6210.21729\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 17094.6074 - val_loss: 15294.1025 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 18994.5977\n",
      "Epoch 10: val_loss improved from 6210.21729 to 4378.32764, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 15471.7695 - val_loss: 4378.3276 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5294.1475\n",
      "Epoch 11: val_loss did not improve from 4378.32764\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 11759.7412 - val_loss: 4642.4404 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4675.0850\n",
      "Epoch 12: val_loss improved from 4378.32764 to 3449.10059, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 11174.0449 - val_loss: 3449.1006 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 15602.8945\n",
      "Epoch 13: val_loss did not improve from 3449.10059\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 10904.3545 - val_loss: 3549.1487 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10681.0508\n",
      "Epoch 14: val_loss improved from 3449.10059 to 2742.15332, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 8811.6768 - val_loss: 2742.1533 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 16662.8926\n",
      "Epoch 15: val_loss improved from 2742.15332 to 2632.85156, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 8832.5283 - val_loss: 2632.8516 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4698.7397\n",
      "Epoch 16: val_loss improved from 2632.85156 to 2334.38062, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 8948.1084 - val_loss: 2334.3806 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5109.5283\n",
      "Epoch 17: val_loss improved from 2334.38062 to 2284.66553, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 8387.5117 - val_loss: 2284.6655 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9460.9717\n",
      "Epoch 18: val_loss improved from 2284.66553 to 2095.93530, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 8867.7930 - val_loss: 2095.9353 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7008.0449\n",
      "Epoch 19: val_loss improved from 2095.93530 to 1877.81580, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 9443.3086 - val_loss: 1877.8158 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7914.3369\n",
      "Epoch 20: val_loss did not improve from 1877.81580\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7647.0039 - val_loss: 1915.9591 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3594.8921\n",
      "Epoch 21: val_loss improved from 1877.81580 to 1709.32434, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 7245.0020 - val_loss: 1709.3243 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3189.8354\n",
      "Epoch 22: val_loss improved from 1709.32434 to 1659.08374, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 5873.9937 - val_loss: 1659.0837 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - ETA: 0s - loss: 7059.4272\n",
      "Epoch 23: val_loss did not improve from 1659.08374\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 7059.4272 - val_loss: 1665.5157 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5184.7734\n",
      "Epoch 24: val_loss improved from 1659.08374 to 1634.25256, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 7648.8389 - val_loss: 1634.2526 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4814.8447\n",
      "Epoch 25: val_loss improved from 1634.25256 to 1503.39697, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 7719.6382 - val_loss: 1503.3970 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9858.2051\n",
      "Epoch 26: val_loss did not improve from 1503.39697\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5888.6304 - val_loss: 1595.1198 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6818.0884\n",
      "Epoch 27: val_loss did not improve from 1503.39697\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6621.2798 - val_loss: 1518.1357 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8426.2764\n",
      "Epoch 28: val_loss improved from 1503.39697 to 1466.16663, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 8529.7881 - val_loss: 1466.1666 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10798.1133\n",
      "Epoch 29: val_loss improved from 1466.16663 to 1404.52478, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 7068.2710 - val_loss: 1404.5248 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7024.5420\n",
      "Epoch 30: val_loss improved from 1404.52478 to 1322.71313, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 6687.3384 - val_loss: 1322.7131 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5938.7695\n",
      "Epoch 31: val_loss did not improve from 1322.71313\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6891.4873 - val_loss: 1403.0278 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8085.4746\n",
      "Epoch 32: val_loss improved from 1322.71313 to 1248.78027, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 5906.3652 - val_loss: 1248.7803 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4935.8271\n",
      "Epoch 33: val_loss did not improve from 1248.78027\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7458.3682 - val_loss: 1559.9485 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7673.4463\n",
      "Epoch 34: val_loss did not improve from 1248.78027\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6620.2876 - val_loss: 1654.7571 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7439.7158\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1248.78027\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 6396.6626 - val_loss: 1447.7853 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6685.3096\n",
      "Epoch 36: val_loss improved from 1248.78027 to 1218.30359, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 6328.4546 - val_loss: 1218.3036 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5694.1592\n",
      "Epoch 37: val_loss improved from 1218.30359 to 1116.19885, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 6480.5679 - val_loss: 1116.1989 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4020.4504\n",
      "Epoch 38: val_loss did not improve from 1116.19885\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6964.9629 - val_loss: 1149.9064 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 16843.9180\n",
      "Epoch 39: val_loss did not improve from 1116.19885\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 9451.3242 - val_loss: 1159.0823 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2667.4307\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1116.19885\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6039.4800 - val_loss: 1161.8381 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5220.2671\n",
      "Epoch 41: val_loss improved from 1116.19885 to 1092.81079, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 7123.1206 - val_loss: 1092.8108 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9076.6973\n",
      "Epoch 42: val_loss did not improve from 1092.81079\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5660.7065 - val_loss: 1182.6139 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5379.0371\n",
      "Epoch 43: val_loss did not improve from 1092.81079\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6481.9873 - val_loss: 1133.3875 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7530.5967\n",
      "Epoch 44: val_loss improved from 1092.81079 to 1063.68762, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 7360.3442 - val_loss: 1063.6876 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 14991.1133\n",
      "Epoch 45: val_loss did not improve from 1063.68762\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8279.8184 - val_loss: 1067.0339 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7459.1807\n",
      "Epoch 46: val_loss did not improve from 1063.68762\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6180.2568 - val_loss: 1068.5178 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7588.0488\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1063.68762\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6164.9058 - val_loss: 1081.9702 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5312.6475\n",
      "Epoch 48: val_loss did not improve from 1063.68762\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5540.2251 - val_loss: 1065.8005 - lr: 1.2500e-04\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4695.8804\n",
      "Epoch 49: val_loss improved from 1063.68762 to 1054.13477, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 6866.8120 - val_loss: 1054.1348 - lr: 1.2500e-04\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8355.8008\n",
      "Epoch 50: val_loss improved from 1054.13477 to 1031.73889, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 5768.2627 - val_loss: 1031.7389 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2234.3506\n",
      "Epoch 51: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 5577.2119 - val_loss: 1039.4243 - lr: 1.2500e-04\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8985.5195\n",
      "Epoch 52: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6544.8652 - val_loss: 1085.9554 - lr: 1.2500e-04\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 13503.8574\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 6895.3228 - val_loss: 1063.0183 - lr: 1.2500e-04\n",
      "Epoch 54/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3065.5276\n",
      "Epoch 54: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6127.8042 - val_loss: 1050.0179 - lr: 6.2500e-05\n",
      "Epoch 55/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8687.7578\n",
      "Epoch 55: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5175.4482 - val_loss: 1052.6575 - lr: 6.2500e-05\n",
      "Epoch 56/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3834.4126\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7070.8403 - val_loss: 1062.0133 - lr: 6.2500e-05\n",
      "Epoch 57/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5614.9668\n",
      "Epoch 57: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 8025.0146 - val_loss: 1079.2520 - lr: 3.1250e-05\n",
      "Epoch 58/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2747.7576\n",
      "Epoch 58: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6357.0767 - val_loss: 1100.8959 - lr: 3.1250e-05\n",
      "Epoch 59/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6165.4043\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 59: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 5938.7510 - val_loss: 1124.4156 - lr: 3.1250e-05\n",
      "Epoch 60/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4793.8604Restoring model weights from the end of the best epoch: 50.\n",
      "\n",
      "Epoch 60: val_loss did not improve from 1031.73889\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5405.2954 - val_loss: 1155.3174 - lr: 1.5625e-05\n",
      "Epoch 60: early stopping\n",
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 4s - loss: 265017.9375\n",
      "Epoch 1: val_loss improved from inf to 256851.23438, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 44ms/step - loss: 265854.8125 - val_loss: 256851.2344 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 181193.2188\n",
      "Epoch 2: val_loss improved from 256851.23438 to 256035.32812, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 265315.9688 - val_loss: 256035.3281 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 228683.9062\n",
      "Epoch 3: val_loss improved from 256035.32812 to 253695.73438, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 263960.8750 - val_loss: 253695.7344 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 324641.2812\n",
      "Epoch 4: val_loss improved from 253695.73438 to 247056.56250, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 260251.2188 - val_loss: 247056.5625 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 336524.9375\n",
      "Epoch 5: val_loss improved from 247056.56250 to 229438.73438, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 249413.2188 - val_loss: 229438.7344 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 225652.7812\n",
      "Epoch 6: val_loss improved from 229438.73438 to 186841.42188, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 221350.2344 - val_loss: 186841.4219 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 186561.1875\n",
      "Epoch 7: val_loss improved from 186841.42188 to 105497.05469, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 160756.2188 - val_loss: 105497.0547 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 112132.3828\n",
      "Epoch 8: val_loss improved from 105497.05469 to 15642.12891, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 63798.3555 - val_loss: 15642.1289 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 18783.4023\n",
      "Epoch 9: val_loss did not improve from 15642.12891\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 14533.7881 - val_loss: 16267.3672 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 20171.6445\n",
      "Epoch 10: val_loss improved from 15642.12891 to 5637.32617, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 17999.4570 - val_loss: 5637.3262 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6030.0005\n",
      "Epoch 11: val_loss did not improve from 5637.32617\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 9910.4512 - val_loss: 8389.0791 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12978.9551\n",
      "Epoch 12: val_loss improved from 5637.32617 to 4941.70215, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 10104.9941 - val_loss: 4941.7021 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6020.7695\n",
      "Epoch 13: val_loss improved from 4941.70215 to 4120.85498, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 8776.8447 - val_loss: 4120.8550 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8696.2305\n",
      "Epoch 14: val_loss improved from 4120.85498 to 3426.03638, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 10663.3555 - val_loss: 3426.0364 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4455.2588\n",
      "Epoch 15: val_loss improved from 3426.03638 to 3002.80322, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 8481.2080 - val_loss: 3002.8032 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5685.6709\n",
      "Epoch 16: val_loss improved from 3002.80322 to 2675.11304, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 8547.0625 - val_loss: 2675.1130 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10419.0410\n",
      "Epoch 17: val_loss did not improve from 2675.11304\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 8614.7998 - val_loss: 2738.9839 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11613.4043\n",
      "Epoch 18: val_loss improved from 2675.11304 to 2325.83325, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 7687.5874 - val_loss: 2325.8333 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9852.3184\n",
      "Epoch 19: val_loss improved from 2325.83325 to 2038.71252, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 7709.9805 - val_loss: 2038.7125 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6310.0996\n",
      "Epoch 20: val_loss improved from 2038.71252 to 1933.29712, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 6091.6343 - val_loss: 1933.2971 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6121.2207\n",
      "Epoch 21: val_loss improved from 1933.29712 to 1899.01196, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 7575.4351 - val_loss: 1899.0120 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4746.1333\n",
      "Epoch 22: val_loss improved from 1899.01196 to 1818.72095, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 7390.0508 - val_loss: 1818.7209 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5382.5869\n",
      "Epoch 23: val_loss improved from 1818.72095 to 1663.01013, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 6728.6089 - val_loss: 1663.0101 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8136.6777\n",
      "Epoch 24: val_loss did not improve from 1663.01013\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 9099.8477 - val_loss: 2524.1274 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12314.9355\n",
      "Epoch 25: val_loss improved from 1663.01013 to 1530.79761, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 7176.5581 - val_loss: 1530.7976 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6981.4336\n",
      "Epoch 26: val_loss improved from 1530.79761 to 1474.31958, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 5824.9761 - val_loss: 1474.3196 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9412.2012\n",
      "Epoch 27: val_loss did not improve from 1474.31958\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6343.5620 - val_loss: 1570.5056 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4360.5308\n",
      "Epoch 28: val_loss improved from 1474.31958 to 1401.02747, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 6379.0054 - val_loss: 1401.0275 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8705.4600\n",
      "Epoch 29: val_loss improved from 1401.02747 to 1370.82556, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 5836.7021 - val_loss: 1370.8256 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5197.7983\n",
      "Epoch 30: val_loss did not improve from 1370.82556\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6258.1689 - val_loss: 1826.2773 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5203.3662\n",
      "Epoch 31: val_loss did not improve from 1370.82556\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5833.1313 - val_loss: 1376.4963 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2011.3296\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1370.82556\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5197.8184 - val_loss: 1376.9391 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6128.6836\n",
      "Epoch 33: val_loss improved from 1370.82556 to 1292.12085, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 5148.2534 - val_loss: 1292.1208 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4236.6055\n",
      "Epoch 34: val_loss did not improve from 1292.12085\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 4724.7305 - val_loss: 1516.7441 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5164.3867\n",
      "Epoch 35: val_loss did not improve from 1292.12085\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 6820.3530 - val_loss: 1366.1169 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3237.2864\n",
      "Epoch 36: val_loss improved from 1292.12085 to 1233.41931, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 6697.8379 - val_loss: 1233.4193 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3974.1843\n",
      "Epoch 37: val_loss improved from 1233.41931 to 1210.97253, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 5743.7178 - val_loss: 1210.9725 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2857.4885\n",
      "Epoch 38: val_loss improved from 1210.97253 to 1187.55054, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 5757.1616 - val_loss: 1187.5505 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8132.4570\n",
      "Epoch 39: val_loss did not improve from 1187.55054\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5492.9370 - val_loss: 1210.9259 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4270.1230\n",
      "Epoch 40: val_loss did not improve from 1187.55054\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7827.9814 - val_loss: 1434.9272 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5729.5342\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1187.55054\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7093.8857 - val_loss: 1327.6376 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5226.0938\n",
      "Epoch 42: val_loss improved from 1187.55054 to 1163.14270, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 4938.8125 - val_loss: 1163.1427 - lr: 2.5000e-04\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3707.2832\n",
      "Epoch 43: val_loss improved from 1163.14270 to 1132.80396, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 6008.5825 - val_loss: 1132.8040 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9638.0127\n",
      "Epoch 44: val_loss improved from 1132.80396 to 1124.35962, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 6766.0264 - val_loss: 1124.3596 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5778.2383\n",
      "Epoch 45: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 6515.3486 - val_loss: 1152.6874 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5920.2197\n",
      "Epoch 46: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5888.8091 - val_loss: 1222.0795 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7139.3315\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6705.7480 - val_loss: 1170.3438 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6129.8105\n",
      "Epoch 48: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6321.3911 - val_loss: 1178.9669 - lr: 1.2500e-04\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11691.0254\n",
      "Epoch 49: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6794.6133 - val_loss: 1163.3193 - lr: 1.2500e-04\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6607.0767\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7074.6465 - val_loss: 1125.2334 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4969.6733\n",
      "Epoch 51: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6809.5659 - val_loss: 1130.8438 - lr: 6.2500e-05\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2520.2886\n",
      "Epoch 52: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5652.3193 - val_loss: 1142.0367 - lr: 6.2500e-05\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4064.9229\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 6068.5581 - val_loss: 1165.9343 - lr: 6.2500e-05\n",
      "Epoch 54/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7406.7578Restoring model weights from the end of the best epoch: 44.\n",
      "\n",
      "Epoch 54: val_loss did not improve from 1124.35962\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6181.8945 - val_loss: 1206.0261 - lr: 3.1250e-05\n",
      "Epoch 54: early stopping\n",
      "Epoch 1/200\n",
      "1/9 [==>...........................] - ETA: 5s - loss: 275486.6875\n",
      "Epoch 1: val_loss improved from inf to 257775.51562, saving model to best_model.h5\n",
      "9/9 [==============================] - 1s 53ms/step - loss: 265520.0312 - val_loss: 257775.5156 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 274139.0625\n",
      "Epoch 2: val_loss improved from 257775.51562 to 256452.56250, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 264775.7188 - val_loss: 256452.5625 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 316272.0312\n",
      "Epoch 3: val_loss improved from 256452.56250 to 252528.35938, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 262638.9062 - val_loss: 252528.3594 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 350346.9375\n",
      "Epoch 4: val_loss improved from 252528.35938 to 241447.34375, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 256551.9375 - val_loss: 241447.3438 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 341877.4062\n",
      "Epoch 5: val_loss improved from 241447.34375 to 213154.95312, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 239119.5312 - val_loss: 213154.9531 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 222851.6094\n",
      "Epoch 6: val_loss improved from 213154.95312 to 151929.26562, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 194820.3281 - val_loss: 151929.2656 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 159479.4531\n",
      "Epoch 7: val_loss improved from 151929.26562 to 57271.34375, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 117154.6875 - val_loss: 57271.3438 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 62935.6484\n",
      "Epoch 8: val_loss improved from 57271.34375 to 8019.49365, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 32434.2324 - val_loss: 8019.4937 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12956.8906\n",
      "Epoch 9: val_loss did not improve from 8019.49365\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 17279.7031 - val_loss: 12430.3730 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 21714.5195\n",
      "Epoch 10: val_loss improved from 8019.49365 to 7238.54102, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 11772.1387 - val_loss: 7238.5410 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11265.1270\n",
      "Epoch 11: val_loss did not improve from 7238.54102\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 12919.7734 - val_loss: 8003.1147 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11355.0078\n",
      "Epoch 12: val_loss improved from 7238.54102 to 4807.70752, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 10901.9209 - val_loss: 4807.7075 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8835.2188\n",
      "Epoch 13: val_loss improved from 4807.70752 to 4574.63916, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 7755.4082 - val_loss: 4574.6392 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7042.2520\n",
      "Epoch 14: val_loss improved from 4574.63916 to 4461.59912, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 24ms/step - loss: 9439.7324 - val_loss: 4461.5991 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11689.0322\n",
      "Epoch 15: val_loss did not improve from 4461.59912\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 10579.0146 - val_loss: 4842.2153 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10171.0518\n",
      "Epoch 16: val_loss improved from 4461.59912 to 3659.67480, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 8256.8047 - val_loss: 3659.6748 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3997.1045\n",
      "Epoch 17: val_loss improved from 3659.67480 to 3485.55078, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 8321.5127 - val_loss: 3485.5508 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11755.0566\n",
      "Epoch 18: val_loss did not improve from 3485.55078\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7783.2480 - val_loss: 3574.9395 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 11018.9941\n",
      "Epoch 19: val_loss improved from 3485.55078 to 3346.82617, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 6918.4731 - val_loss: 3346.8262 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7386.9688\n",
      "Epoch 20: val_loss improved from 3346.82617 to 3010.08398, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 5829.7866 - val_loss: 3010.0840 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4125.7812\n",
      "Epoch 21: val_loss improved from 3010.08398 to 2765.08936, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 7122.4336 - val_loss: 2765.0894 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3548.4678\n",
      "Epoch 22: val_loss improved from 2765.08936 to 2756.20020, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 5816.2764 - val_loss: 2756.2002 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6549.2031\n",
      "Epoch 23: val_loss improved from 2756.20020 to 2644.47290, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 7269.9756 - val_loss: 2644.4729 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5977.2681\n",
      "Epoch 24: val_loss improved from 2644.47290 to 2426.11597, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 7086.8618 - val_loss: 2426.1160 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5359.3027\n",
      "Epoch 25: val_loss improved from 2426.11597 to 2398.55640, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 7437.4482 - val_loss: 2398.5564 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5904.3560\n",
      "Epoch 26: val_loss improved from 2398.55640 to 2271.31348, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 7198.0723 - val_loss: 2271.3135 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7665.0376\n",
      "Epoch 27: val_loss did not improve from 2271.31348\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7089.5278 - val_loss: 2836.9883 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8517.0078\n",
      "Epoch 28: val_loss did not improve from 2271.31348\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7710.9521 - val_loss: 2652.2341 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7434.1479\n",
      "Epoch 29: val_loss improved from 2271.31348 to 2089.96338, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 6035.1987 - val_loss: 2089.9634 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 4140.3057\n",
      "Epoch 30: val_loss did not improve from 2089.96338\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6212.4629 - val_loss: 2120.4119 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10435.2402\n",
      "Epoch 31: val_loss did not improve from 2089.96338\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7162.0684 - val_loss: 2264.8694 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7201.6084\n",
      "Epoch 32: val_loss improved from 2089.96338 to 1945.64563, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 6470.1831 - val_loss: 1945.6456 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 9844.8799\n",
      "Epoch 33: val_loss did not improve from 1945.64563\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5738.0454 - val_loss: 2038.7628 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8517.1680\n",
      "Epoch 34: val_loss did not improve from 1945.64563\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5418.7559 - val_loss: 2177.3826 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "6/9 [===================>..........] - ETA: 0s - loss: 5047.9282\n",
      "Epoch 35: val_loss improved from 1945.64563 to 1860.56116, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 29ms/step - loss: 5911.7529 - val_loss: 1860.5612 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3302.0298\n",
      "Epoch 36: val_loss improved from 1860.56116 to 1794.14771, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 4912.7759 - val_loss: 1794.1477 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5459.1470\n",
      "Epoch 37: val_loss improved from 1794.14771 to 1731.46375, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 25ms/step - loss: 5939.5073 - val_loss: 1731.4637 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6862.3193\n",
      "Epoch 38: val_loss did not improve from 1731.46375\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 7319.3271 - val_loss: 1900.1581 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 16294.6992\n",
      "Epoch 39: val_loss did not improve from 1731.46375\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 7789.1040 - val_loss: 1918.1697 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7365.6641\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1731.46375\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 8364.0918 - val_loss: 1835.0312 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 5295.6514\n",
      "Epoch 41: val_loss did not improve from 1731.46375\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5141.0913 - val_loss: 1771.2510 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2392.2085\n",
      "Epoch 42: val_loss improved from 1731.46375 to 1609.30554, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 28ms/step - loss: 4374.1812 - val_loss: 1609.3055 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2616.8125\n",
      "Epoch 43: val_loss improved from 1609.30554 to 1590.46301, saving model to best_model.h5\n",
      "9/9 [==============================] - 0s 22ms/step - loss: 4923.0981 - val_loss: 1590.4630 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 8991.6504\n",
      "Epoch 44: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6124.8755 - val_loss: 2005.2106 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3660.2937\n",
      "Epoch 45: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5211.1709 - val_loss: 1815.9976 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3373.0583\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5782.7686 - val_loss: 1757.2123 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 7259.4307\n",
      "Epoch 47: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5482.9419 - val_loss: 1692.9336 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 10405.5049\n",
      "Epoch 48: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5865.7695 - val_loss: 1707.0973 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6415.7974\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5628.1992 - val_loss: 1623.6844 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 12653.2490\n",
      "Epoch 50: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5806.0967 - val_loss: 1642.4554 - lr: 1.2500e-04\n",
      "Epoch 51/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 3883.3606\n",
      "Epoch 51: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 5670.0898 - val_loss: 1752.2455 - lr: 1.2500e-04\n",
      "Epoch 52/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 6404.5054\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 52: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 5223.9258 - val_loss: 1889.2529 - lr: 1.2500e-04\n",
      "Epoch 53/200\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 2572.1846Restoring model weights from the end of the best epoch: 43.\n",
      "\n",
      "Epoch 53: val_loss did not improve from 1590.46301\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 6066.3916 - val_loss: 1931.9846 - lr: 6.2500e-05\n",
      "Epoch 53: early stopping\n",
      "Validation MSE scores for each fold: [1684.8548583984375, 2054.017822265625, 1031.7388916015625, 1124.359619140625, 1590.4630126953125]\n",
      "Mean Validation MSE: 1497.0868408203125\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_train_scaled and y_train are NumPy arrays\n",
    "X_train_scaled = np.array(X_train_scaled)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores = []\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "    \n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=200, batch_size=32, \n",
    "                        validation_data=(X_val_fold, y_val_fold), verbose=1, \n",
    "                        callbacks=[lr_scheduler, early_stopping, checkpoint])\n",
    "    \n",
    "    val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    val_scores.append(val_loss)\n",
    "\n",
    "print(f'Validation MSE scores for each fold: {val_scores}')\n",
    "print(f'Mean Validation MSE: {np.mean(val_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best weights\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Assuming you have X_test_scaled and y_test as your test dataset\n",
    "# Replace these with your actual test datasets\n",
    "X_test_scaled = np.array(X_test_scaled)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict the prices on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Plot Predicted vs Actual Prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Diagonal line\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('trained_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
