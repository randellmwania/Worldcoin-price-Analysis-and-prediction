{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Randell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Randell\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Market Cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-16</td>\n",
       "      <td>2024-06-17</td>\n",
       "      <td>435.434564</td>\n",
       "      <td>450.283266</td>\n",
       "      <td>424.506973</td>\n",
       "      <td>444.698612</td>\n",
       "      <td>3.149406e+10</td>\n",
       "      <td>9.786989e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>2024-06-16</td>\n",
       "      <td>442.207514</td>\n",
       "      <td>450.754333</td>\n",
       "      <td>433.511423</td>\n",
       "      <td>435.805905</td>\n",
       "      <td>4.558511e+10</td>\n",
       "      <td>9.913209e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>2024-06-15</td>\n",
       "      <td>471.470823</td>\n",
       "      <td>479.991323</td>\n",
       "      <td>431.186058</td>\n",
       "      <td>442.021938</td>\n",
       "      <td>4.505473e+10</td>\n",
       "      <td>1.028360e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-13</td>\n",
       "      <td>2024-06-14</td>\n",
       "      <td>494.564923</td>\n",
       "      <td>500.807162</td>\n",
       "      <td>466.376275</td>\n",
       "      <td>468.211321</td>\n",
       "      <td>5.922474e+10</td>\n",
       "      <td>1.073532e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-12</td>\n",
       "      <td>2024-06-13</td>\n",
       "      <td>489.541653</td>\n",
       "      <td>526.686680</td>\n",
       "      <td>469.800441</td>\n",
       "      <td>497.223618</td>\n",
       "      <td>7.096327e+10</td>\n",
       "      <td>1.118204e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>266.303801</td>\n",
       "      <td>286.533403</td>\n",
       "      <td>264.554328</td>\n",
       "      <td>285.644185</td>\n",
       "      <td>6.007178e+10</td>\n",
       "      <td>2.919856e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>2023-07-28</td>\n",
       "      <td>287.676920</td>\n",
       "      <td>295.834139</td>\n",
       "      <td>268.693036</td>\n",
       "      <td>269.989743</td>\n",
       "      <td>9.477407e+10</td>\n",
       "      <td>2.970630e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>2023-07-27</td>\n",
       "      <td>293.852710</td>\n",
       "      <td>321.444574</td>\n",
       "      <td>282.445740</td>\n",
       "      <td>289.173453</td>\n",
       "      <td>1.894057e+11</td>\n",
       "      <td>3.163373e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>2023-07-26</td>\n",
       "      <td>278.619394</td>\n",
       "      <td>309.644293</td>\n",
       "      <td>245.956788</td>\n",
       "      <td>293.075495</td>\n",
       "      <td>1.737368e+11</td>\n",
       "      <td>2.890207e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2023-07-24</td>\n",
       "      <td>2023-07-25</td>\n",
       "      <td>232.927358</td>\n",
       "      <td>414.103163</td>\n",
       "      <td>227.284983</td>\n",
       "      <td>283.325462</td>\n",
       "      <td>6.015643e+10</td>\n",
       "      <td>2.903896e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Start         End        Open        High         Low       Close  \\\n",
       "0    2024-06-16  2024-06-17  435.434564  450.283266  424.506973  444.698612   \n",
       "1    2024-06-15  2024-06-16  442.207514  450.754333  433.511423  435.805905   \n",
       "2    2024-06-14  2024-06-15  471.470823  479.991323  431.186058  442.021938   \n",
       "3    2024-06-13  2024-06-14  494.564923  500.807162  466.376275  468.211321   \n",
       "4    2024-06-12  2024-06-13  489.541653  526.686680  469.800441  497.223618   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "324  2023-07-28  2023-07-29  266.303801  286.533403  264.554328  285.644185   \n",
       "325  2023-07-27  2023-07-28  287.676920  295.834139  268.693036  269.989743   \n",
       "326  2023-07-26  2023-07-27  293.852710  321.444574  282.445740  289.173453   \n",
       "327  2023-07-25  2023-07-26  278.619394  309.644293  245.956788  293.075495   \n",
       "328  2023-07-24  2023-07-25  232.927358  414.103163  227.284983  283.325462   \n",
       "\n",
       "           Volume    Market Cap  \n",
       "0    3.149406e+10  9.786989e+10  \n",
       "1    4.558511e+10  9.913209e+10  \n",
       "2    4.505473e+10  1.028360e+11  \n",
       "3    5.922474e+10  1.073532e+11  \n",
       "4    7.096327e+10  1.118204e+11  \n",
       "..            ...           ...  \n",
       "324  6.007178e+10  2.919856e+10  \n",
       "325  9.477407e+10  2.970630e+10  \n",
       "326  1.894057e+11  3.163373e+10  \n",
       "327  1.737368e+11  2.890207e+10  \n",
       "328  6.015643e+10  2.903896e+10  \n",
       "\n",
       "[329 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data reading\n",
    "df = pd.read_csv(\"worldcoin-org_2019-06-19_2024-06-17.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert Start and End columns to datetime\n",
    "df[\"Start\"] = pd.to_datetime(df[\"Start\"])\n",
    "df[\"End\"] = pd.to_datetime(df[\"End\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features and target variable\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\", \"Market Cap\"]]\n",
    "y = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 355299.1562\n",
      "Epoch 1: val_loss improved from inf to 348791.12500, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 63ms/step - loss: 300338.5625 - val_loss: 348791.1250 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 371129.0625\n",
      "Epoch 2: val_loss improved from 348791.12500 to 348594.15625, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 300204.0625 - val_loss: 348594.1562 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 293907.7500\n",
      "Epoch 3: val_loss improved from 348594.15625 to 348118.53125, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 299937.6875 - val_loss: 348118.5312 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 378031.4688\n",
      "Epoch 4: val_loss improved from 348118.53125 to 346949.06250, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 299350.0000 - val_loss: 346949.0625 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 302997.5625\n",
      "Epoch 5: val_loss improved from 346949.06250 to 344162.84375, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 297806.5625 - val_loss: 344162.8438 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 239433.7500\n",
      "Epoch 6: val_loss improved from 344162.84375 to 337760.53125, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 293755.2812 - val_loss: 337760.5312 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 215529.9844\n",
      "Epoch 7: val_loss improved from 337760.53125 to 323707.31250, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 285645.9062 - val_loss: 323707.3125 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 238079.4375\n",
      "Epoch 8: val_loss improved from 323707.31250 to 296103.78125, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 268418.7188 - val_loss: 296103.7812 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 241512.6875\n",
      "Epoch 9: val_loss improved from 296103.78125 to 246257.43750, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 236876.2656 - val_loss: 246257.4375 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 227451.0156\n",
      "Epoch 10: val_loss improved from 246257.43750 to 168053.89062, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 184056.1875 - val_loss: 168053.8906 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 171847.2812\n",
      "Epoch 11: val_loss improved from 168053.89062 to 70442.96094, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 106097.0703 - val_loss: 70442.9609 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 45270.9453\n",
      "Epoch 12: val_loss improved from 70442.96094 to 8360.80859, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 25579.1543 - val_loss: 8360.8086 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5598.5010\n",
      "Epoch 13: val_loss did not improve from 8360.80859\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 15249.2344 - val_loss: 9521.7686 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15113.7529\n",
      "Epoch 14: val_loss improved from 8360.80859 to 5084.82959, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 18132.3945 - val_loss: 5084.8296 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9852.2695\n",
      "Epoch 15: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7804.4292 - val_loss: 9761.1387 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 12556.4854\n",
      "Epoch 16: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 12484.4395 - val_loss: 10262.5381 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 13600.3086\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 10978.9805 - val_loss: 6177.2539 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9815.0049\n",
      "Epoch 18: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7238.6821 - val_loss: 5675.0229 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5275.8828\n",
      "Epoch 19: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 8188.1104 - val_loss: 5114.0742 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6089.5508\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 20: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8369.2354 - val_loss: 5209.6709 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6487.0562\n",
      "Epoch 21: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8059.9521 - val_loss: 5495.9937 - lr: 2.5000e-04\n",
      "Epoch 22/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3073.9053\n",
      "Epoch 22: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8032.5737 - val_loss: 6020.5210 - lr: 2.5000e-04\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 16617.5723\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 23: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 10137.5977 - val_loss: 6174.1543 - lr: 2.5000e-04\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 12761.1699Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 5084.82959\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 9919.1104 - val_loss: 6271.9932 - lr: 1.2500e-04\n",
      "Epoch 24: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Increase Model Complexity\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, \n",
    "                    validation_split=0.2, verbose=1, \n",
    "                    callbacks=[lr_scheduler, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 267519.3750\n",
      "Epoch 1: val_loss improved from inf to 287493.62500, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 55ms/step - loss: 315652.8125 - val_loss: 287493.6250 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 325107.8750\n",
      "Epoch 2: val_loss improved from 287493.62500 to 286941.65625, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 315260.3438 - val_loss: 286941.6562 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 316664.0625\n",
      "Epoch 3: val_loss improved from 286941.65625 to 285607.96875, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 314464.4062 - val_loss: 285607.9688 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 319421.3750\n",
      "Epoch 4: val_loss improved from 285607.96875 to 282524.87500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 312206.0625 - val_loss: 282524.8750 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 249011.8438\n",
      "Epoch 5: val_loss improved from 282524.87500 to 275382.31250, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 307352.5312 - val_loss: 275382.3125 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 321114.5625\n",
      "Epoch 6: val_loss improved from 275382.31250 to 260500.75000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 297276.9375 - val_loss: 260500.7500 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 249601.3750\n",
      "Epoch 7: val_loss improved from 260500.75000 to 231989.01562, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 273933.1875 - val_loss: 231989.0156 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 212971.6250\n",
      "Epoch 8: val_loss improved from 231989.01562 to 181756.98438, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 236865.6250 - val_loss: 181756.9844 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 186740.0000\n",
      "Epoch 9: val_loss improved from 181756.98438 to 106064.48438, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 172638.0625 - val_loss: 106064.4844 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 78977.4609\n",
      "Epoch 10: val_loss improved from 106064.48438 to 26571.52734, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 80676.6641 - val_loss: 26571.5273 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 29432.1270\n",
      "Epoch 11: val_loss improved from 26571.52734 to 6972.06641, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 15498.2061 - val_loss: 6972.0664 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11767.7949\n",
      "Epoch 12: val_loss did not improve from 6972.06641\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 23313.1406 - val_loss: 9867.3877 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 21170.0078\n",
      "Epoch 13: val_loss improved from 6972.06641 to 3711.93799, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 12186.9932 - val_loss: 3711.9380 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11852.4199\n",
      "Epoch 14: val_loss did not improve from 3711.93799\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 13236.4727 - val_loss: 5213.2529 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7176.7422\n",
      "Epoch 15: val_loss improved from 3711.93799 to 3643.23267, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11315.8037 - val_loss: 3643.2327 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 13318.3711\n",
      "Epoch 16: val_loss improved from 3643.23267 to 2766.63208, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 10745.5273 - val_loss: 2766.6321 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6690.5884\n",
      "Epoch 17: val_loss improved from 2766.63208 to 2586.76416, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7683.0010 - val_loss: 2586.7642 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15837.4727\n",
      "Epoch 18: val_loss improved from 2586.76416 to 2366.48828, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9756.6826 - val_loss: 2366.4883 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6126.5596\n",
      "Epoch 19: val_loss did not improve from 2366.48828\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7042.8604 - val_loss: 2616.4390 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6994.5615\n",
      "Epoch 20: val_loss improved from 2366.48828 to 2224.75806, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8816.9277 - val_loss: 2224.7581 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5838.7197\n",
      "Epoch 21: val_loss improved from 2224.75806 to 2013.42175, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6428.8799 - val_loss: 2013.4218 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9337.0059\n",
      "Epoch 22: val_loss did not improve from 2013.42175\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 9300.6299 - val_loss: 2157.7324 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11702.1992\n",
      "Epoch 23: val_loss improved from 2013.42175 to 2007.45898, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8924.2031 - val_loss: 2007.4590 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8487.4199\n",
      "Epoch 24: val_loss improved from 2007.45898 to 1738.92957, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9260.5811 - val_loss: 1738.9296 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10643.5479\n",
      "Epoch 25: val_loss improved from 1738.92957 to 1617.29163, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9847.3145 - val_loss: 1617.2916 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7460.3154\n",
      "Epoch 26: val_loss did not improve from 1617.29163\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 9878.8945 - val_loss: 1913.1072 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5577.9121\n",
      "Epoch 27: val_loss did not improve from 1617.29163\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7925.6714 - val_loss: 1653.7396 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9197.8145\n",
      "Epoch 28: val_loss improved from 1617.29163 to 1558.35864, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9606.7100 - val_loss: 1558.3586 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4741.5605\n",
      "Epoch 29: val_loss did not improve from 1558.35864\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 6979.1011 - val_loss: 1559.1548 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6340.2217\n",
      "Epoch 30: val_loss did not improve from 1558.35864\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8034.1138 - val_loss: 1607.0713 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 2775.8809\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1558.35864\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8385.0039 - val_loss: 1846.4075 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4838.4409\n",
      "Epoch 32: val_loss did not improve from 1558.35864\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 4657.1323 - val_loss: 1769.8516 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6562.3599\n",
      "Epoch 33: val_loss improved from 1558.35864 to 1552.07080, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6986.0977 - val_loss: 1552.0708 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 18116.4414\n",
      "Epoch 34: val_loss improved from 1552.07080 to 1488.55017, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8895.5781 - val_loss: 1488.5502 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 16679.5332\n",
      "Epoch 35: val_loss improved from 1488.55017 to 1396.33386, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8415.3994 - val_loss: 1396.3339 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5991.4727\n",
      "Epoch 36: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7937.4722 - val_loss: 1406.5306 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7328.3018\n",
      "Epoch 37: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8817.0752 - val_loss: 1616.5615 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10570.8730\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8067.1357 - val_loss: 1513.9004 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 18978.7090\n",
      "Epoch 39: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 8392.6572 - val_loss: 1590.2289 - lr: 2.5000e-04\n",
      "Epoch 40/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9033.6787\n",
      "Epoch 40: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 8147.8760 - val_loss: 1687.3344 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 17189.7539\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 10894.2383 - val_loss: 1707.3679 - lr: 2.5000e-04\n",
      "Epoch 42/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6202.1875\n",
      "Epoch 42: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 6538.7725 - val_loss: 1735.2767 - lr: 1.2500e-04\n",
      "Epoch 43/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5892.6582\n",
      "Epoch 43: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 7253.4893 - val_loss: 1701.3656 - lr: 1.2500e-04\n",
      "Epoch 44/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4805.1543\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8491.2139 - val_loss: 1669.2806 - lr: 1.2500e-04\n",
      "Epoch 45/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10355.4160Restoring model weights from the end of the best epoch: 35.\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1396.33386\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 7700.3472 - val_loss: 1715.6852 - lr: 6.2500e-05\n",
      "Epoch 45: early stopping\n",
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 301353.4375\n",
      "Epoch 1: val_loss improved from inf to 255347.43750, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 55ms/step - loss: 323572.7188 - val_loss: 255347.4375 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 330904.6250\n",
      "Epoch 2: val_loss improved from 255347.43750 to 254368.48438, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 322808.6875 - val_loss: 254368.4844 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 278925.6875\n",
      "Epoch 3: val_loss improved from 254368.48438 to 252207.87500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 321169.8750 - val_loss: 252207.8750 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 347209.5312\n",
      "Epoch 4: val_loss improved from 252207.87500 to 247452.51562, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 317609.1250 - val_loss: 247452.5156 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 276684.4375\n",
      "Epoch 5: val_loss improved from 247452.51562 to 237147.51562, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 308741.3750 - val_loss: 237147.5156 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 209887.5625\n",
      "Epoch 6: val_loss improved from 237147.51562 to 215945.12500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 291444.7812 - val_loss: 215945.1250 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 279245.3125\n",
      "Epoch 7: val_loss improved from 215945.12500 to 176430.87500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 254523.0781 - val_loss: 176430.8750 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 139248.2344\n",
      "Epoch 8: val_loss improved from 176430.87500 to 114072.92188, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 196049.2188 - val_loss: 114072.9219 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 187296.6250\n",
      "Epoch 9: val_loss improved from 114072.92188 to 38386.59766, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 115380.6094 - val_loss: 38386.5977 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 62529.7227\n",
      "Epoch 10: val_loss improved from 38386.59766 to 2423.47266, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 31634.2617 - val_loss: 2423.4727 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15029.3574\n",
      "Epoch 11: val_loss did not improve from 2423.47266\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 21953.9980 - val_loss: 14435.0732 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 22393.8418\n",
      "Epoch 12: val_loss improved from 2423.47266 to 2409.11646, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 16602.3438 - val_loss: 2409.1165 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11454.4902\n",
      "Epoch 13: val_loss did not improve from 2409.11646\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 13558.1484 - val_loss: 2515.1985 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 20496.1758\n",
      "Epoch 14: val_loss improved from 2409.11646 to 2145.99561, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11163.7188 - val_loss: 2145.9956 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9019.7227\n",
      "Epoch 15: val_loss improved from 2145.99561 to 1438.32190, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8869.1758 - val_loss: 1438.3219 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10139.9883\n",
      "Epoch 16: val_loss improved from 1438.32190 to 1323.95398, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8109.3799 - val_loss: 1323.9540 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6372.3188\n",
      "Epoch 17: val_loss improved from 1323.95398 to 1169.39221, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7824.1191 - val_loss: 1169.3922 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4062.6289\n",
      "Epoch 18: val_loss did not improve from 1169.39221\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8160.9927 - val_loss: 1266.1315 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6770.5977\n",
      "Epoch 19: val_loss improved from 1169.39221 to 1005.81824, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8186.4463 - val_loss: 1005.8182 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 24728.5781\n",
      "Epoch 20: val_loss improved from 1005.81824 to 914.53064, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 12107.3203 - val_loss: 914.5306 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8329.9883\n",
      "Epoch 21: val_loss improved from 914.53064 to 853.70282, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7595.9331 - val_loss: 853.7028 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "2/7 [=======>......................] - ETA: 0s - loss: 11398.4688\n",
      "Epoch 22: val_loss improved from 853.70282 to 793.80499, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 8170.6768 - val_loss: 793.8050 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10709.7178\n",
      "Epoch 23: val_loss improved from 793.80499 to 737.64105, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7926.6763 - val_loss: 737.6411 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11541.9922\n",
      "Epoch 24: val_loss improved from 737.64105 to 697.43353, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9539.8633 - val_loss: 697.4335 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3851.6055\n",
      "Epoch 25: val_loss did not improve from 697.43353\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8380.8486 - val_loss: 701.1715 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6138.8721\n",
      "Epoch 26: val_loss improved from 697.43353 to 651.76672, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 9126.0322 - val_loss: 651.7667 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6740.5820\n",
      "Epoch 27: val_loss did not improve from 651.76672\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7055.8579 - val_loss: 670.7188 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9766.7559\n",
      "Epoch 28: val_loss improved from 651.76672 to 632.23181, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6952.7075 - val_loss: 632.2318 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11129.6494\n",
      "Epoch 29: val_loss improved from 632.23181 to 627.29639, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7660.6777 - val_loss: 627.2964 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6839.4170\n",
      "Epoch 30: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8270.3359 - val_loss: 684.7504 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 12924.2227\n",
      "Epoch 31: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7270.3857 - val_loss: 710.0704 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6952.7549\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8257.6436 - val_loss: 1103.1617 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3244.4353\n",
      "Epoch 33: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8709.1992 - val_loss: 787.5596 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7383.4424\n",
      "Epoch 34: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 10732.0977 - val_loss: 715.7838 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8436.1191\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8826.9092 - val_loss: 852.8239 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 13311.9287\n",
      "Epoch 36: val_loss did not improve from 627.29639\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8549.6631 - val_loss: 718.5409 - lr: 2.5000e-04\n",
      "Epoch 37/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4029.1638\n",
      "Epoch 37: val_loss improved from 627.29639 to 612.07666, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 6873.9263 - val_loss: 612.0767 - lr: 2.5000e-04\n",
      "Epoch 38/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11455.3711\n",
      "Epoch 38: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7774.0684 - val_loss: 680.5035 - lr: 2.5000e-04\n",
      "Epoch 39/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7404.8833\n",
      "Epoch 39: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 9053.8809 - val_loss: 732.2643 - lr: 2.5000e-04\n",
      "Epoch 40/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 2378.2896\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7719.8364 - val_loss: 690.5385 - lr: 2.5000e-04\n",
      "Epoch 41/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5391.0234\n",
      "Epoch 41: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7086.8174 - val_loss: 697.0540 - lr: 1.2500e-04\n",
      "Epoch 42/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9812.4219\n",
      "Epoch 42: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7569.7905 - val_loss: 684.3168 - lr: 1.2500e-04\n",
      "Epoch 43/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7115.9565\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 43: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 8763.4023 - val_loss: 680.6822 - lr: 1.2500e-04\n",
      "Epoch 44/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6478.7764\n",
      "Epoch 44: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7531.6655 - val_loss: 713.9784 - lr: 6.2500e-05\n",
      "Epoch 45/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4245.3159\n",
      "Epoch 45: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7901.9790 - val_loss: 748.3815 - lr: 6.2500e-05\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - ETA: 0s - loss: 8551.7676 \n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 46: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 8551.7676 - val_loss: 743.7487 - lr: 6.2500e-05\n",
      "Epoch 47/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10358.2910Restoring model weights from the end of the best epoch: 37.\n",
      "\n",
      "Epoch 47: val_loss did not improve from 612.07666\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7995.5479 - val_loss: 750.2613 - lr: 3.1250e-05\n",
      "Epoch 47: early stopping\n",
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 304913.0000\n",
      "Epoch 1: val_loss improved from inf to 380103.81250, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 55ms/step - loss: 292200.5625 - val_loss: 380103.8125 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 327564.4375\n",
      "Epoch 2: val_loss improved from 380103.81250 to 379033.50000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 291676.0312 - val_loss: 379033.5000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 322316.7188\n",
      "Epoch 3: val_loss improved from 379033.50000 to 376550.12500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 290493.0000 - val_loss: 376550.1250 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 265796.7812\n",
      "Epoch 4: val_loss improved from 376550.12500 to 370870.71875, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 287680.9062 - val_loss: 370870.7188 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 390103.3125\n",
      "Epoch 5: val_loss improved from 370870.71875 to 358502.50000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 281590.5938 - val_loss: 358502.5000 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 289328.9375\n",
      "Epoch 6: val_loss improved from 358502.50000 to 332685.09375, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 268842.0000 - val_loss: 332685.0938 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 317899.4688\n",
      "Epoch 7: val_loss improved from 332685.09375 to 283518.50000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 241108.8125 - val_loss: 283518.5000 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 186057.1562\n",
      "Epoch 8: val_loss improved from 283518.50000 to 200782.46875, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 193096.9844 - val_loss: 200782.4688 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 58413.1641\n",
      "Epoch 9: val_loss improved from 200782.46875 to 91159.86719, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 115299.0547 - val_loss: 91159.8672 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 53477.8086\n",
      "Epoch 10: val_loss improved from 91159.86719 to 9235.75781, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 40449.0195 - val_loss: 9235.7578 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 20999.3242\n",
      "Epoch 11: val_loss did not improve from 9235.75781\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 14410.1250 - val_loss: 15748.4199 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 28115.9023\n",
      "Epoch 12: val_loss improved from 9235.75781 to 4551.36182, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 23992.5137 - val_loss: 4551.3618 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7260.8042\n",
      "Epoch 13: val_loss did not improve from 4551.36182\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 9869.6582 - val_loss: 7794.2710 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4859.5591\n",
      "Epoch 14: val_loss did not improve from 4551.36182\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 12764.4170 - val_loss: 7885.3975 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 12977.1367\n",
      "Epoch 15: val_loss improved from 4551.36182 to 4120.96240, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11013.1367 - val_loss: 4120.9624 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10599.3828\n",
      "Epoch 16: val_loss improved from 4120.96240 to 3450.12036, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7287.2998 - val_loss: 3450.1204 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 17899.0684\n",
      "Epoch 17: val_loss did not improve from 3450.12036\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8120.1484 - val_loss: 3455.5757 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9784.1797\n",
      "Epoch 18: val_loss did not improve from 3450.12036\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8102.7739 - val_loss: 3525.7996 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9314.1543\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 19: val_loss did not improve from 3450.12036\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7803.2876 - val_loss: 4168.7178 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8762.7002\n",
      "Epoch 20: val_loss did not improve from 3450.12036\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 5965.8257 - val_loss: 3933.0312 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15081.8047\n",
      "Epoch 21: val_loss did not improve from 3450.12036\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 9009.2021 - val_loss: 3481.0750 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6743.5566\n",
      "Epoch 22: val_loss improved from 3450.12036 to 3230.69214, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8572.0234 - val_loss: 3230.6921 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5399.1055\n",
      "Epoch 23: val_loss improved from 3230.69214 to 3154.94922, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8404.7236 - val_loss: 3154.9492 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9064.7100\n",
      "Epoch 24: val_loss improved from 3154.94922 to 2966.42065, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7765.0254 - val_loss: 2966.4207 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8458.1328\n",
      "Epoch 25: val_loss improved from 2966.42065 to 2949.96313, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8236.1387 - val_loss: 2949.9631 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5644.9951\n",
      "Epoch 26: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6407.0273 - val_loss: 3159.7087 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3187.4663\n",
      "Epoch 27: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 6032.7114 - val_loss: 3278.3506 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4712.2578\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 28: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7405.7563 - val_loss: 3146.7241 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4979.8848\n",
      "Epoch 29: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6055.1733 - val_loss: 3123.2161 - lr: 2.5000e-04\n",
      "Epoch 30/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6698.5088\n",
      "Epoch 30: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7597.3179 - val_loss: 3051.2524 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7277.0044\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 31: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5895.8145 - val_loss: 2969.3406 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10464.2266\n",
      "Epoch 32: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6058.2554 - val_loss: 2966.4612 - lr: 1.2500e-04\n",
      "Epoch 33/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4328.6484\n",
      "Epoch 33: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 5988.7095 - val_loss: 3056.7661 - lr: 1.2500e-04\n",
      "Epoch 34/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4432.8384\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 34: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 5110.6187 - val_loss: 3199.5388 - lr: 1.2500e-04\n",
      "Epoch 35/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9765.8164Restoring model weights from the end of the best epoch: 25.\n",
      "\n",
      "Epoch 35: val_loss did not improve from 2949.96313\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 6909.8633 - val_loss: 3283.2781 - lr: 6.2500e-05\n",
      "Epoch 35: early stopping\n",
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 245524.5312\n",
      "Epoch 1: val_loss improved from inf to 304884.34375, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 55ms/step - loss: 311037.5938 - val_loss: 304884.3438 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 276481.8750\n",
      "Epoch 2: val_loss improved from 304884.34375 to 303761.31250, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 310334.9062 - val_loss: 303761.3125 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 336413.2188\n",
      "Epoch 3: val_loss improved from 303761.31250 to 301122.40625, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 308680.7812 - val_loss: 301122.4062 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 223280.9531\n",
      "Epoch 4: val_loss improved from 301122.40625 to 294961.62500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 304974.8125 - val_loss: 294961.6250 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 231539.4375\n",
      "Epoch 5: val_loss improved from 294961.62500 to 281462.81250, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 295393.3750 - val_loss: 281462.8125 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 277643.0938\n",
      "Epoch 6: val_loss improved from 281462.81250 to 254436.50000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 276752.9062 - val_loss: 254436.5000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 271867.9688\n",
      "Epoch 7: val_loss improved from 254436.50000 to 205323.65625, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 242523.7031 - val_loss: 205323.6562 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 177987.7188\n",
      "Epoch 8: val_loss improved from 205323.65625 to 128669.26562, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 187277.3594 - val_loss: 128669.2656 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 91117.7266\n",
      "Epoch 9: val_loss improved from 128669.26562 to 40472.83203, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 96919.6484 - val_loss: 40472.8320 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 41261.7734\n",
      "Epoch 10: val_loss improved from 40472.83203 to 5017.61133, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 24278.0977 - val_loss: 5017.6113 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7587.5449\n",
      "Epoch 11: val_loss did not improve from 5017.61133\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 17916.8203 - val_loss: 18558.8887 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 19211.3555\n",
      "Epoch 12: val_loss improved from 5017.61133 to 3873.60498, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 17432.4648 - val_loss: 3873.6050 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 13372.3984\n",
      "Epoch 13: val_loss did not improve from 3873.60498\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 11159.0449 - val_loss: 5430.5649 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8434.6846\n",
      "Epoch 14: val_loss did not improve from 3873.60498\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 11421.2402 - val_loss: 4219.6948 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9620.6982\n",
      "Epoch 15: val_loss improved from 3873.60498 to 3024.86938, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 10349.4590 - val_loss: 3024.8694 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 14216.1592\n",
      "Epoch 16: val_loss did not improve from 3024.86938\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8110.2935 - val_loss: 3176.0347 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7374.6001\n",
      "Epoch 17: val_loss improved from 3024.86938 to 2709.72778, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8659.6455 - val_loss: 2709.7278 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15037.7305\n",
      "Epoch 18: val_loss did not improve from 2709.72778\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8197.9600 - val_loss: 2812.5864 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 16075.5527\n",
      "Epoch 19: val_loss improved from 2709.72778 to 2545.34375, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8577.2607 - val_loss: 2545.3438 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3577.4844\n",
      "Epoch 20: val_loss improved from 2545.34375 to 2495.64526, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8324.6982 - val_loss: 2495.6453 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10940.0771\n",
      "Epoch 21: val_loss improved from 2495.64526 to 2423.28418, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 7766.9565 - val_loss: 2423.2842 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5508.6348\n",
      "Epoch 22: val_loss improved from 2423.28418 to 2363.43335, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 6808.2549 - val_loss: 2363.4333 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10056.1035\n",
      "Epoch 23: val_loss improved from 2363.43335 to 2331.00342, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 8522.6299 - val_loss: 2331.0034 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7724.8691\n",
      "Epoch 24: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7138.0322 - val_loss: 2436.2014 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 18705.5762\n",
      "Epoch 25: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8347.2422 - val_loss: 2459.9658 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10944.6641\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8928.3545 - val_loss: 2451.8643 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 6646.0874\n",
      "Epoch 27: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 7046.4170 - val_loss: 2424.1143 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 14136.3936\n",
      "Epoch 28: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7344.4185 - val_loss: 2431.5498 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10859.8887\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7806.3564 - val_loss: 2635.8696 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9273.4111\n",
      "Epoch 30: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7339.3555 - val_loss: 2520.2834 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8938.5205\n",
      "Epoch 31: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 9168.9238 - val_loss: 2410.0205 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7200.4751\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 6303.5884 - val_loss: 2375.0308 - lr: 2.5000e-04\n",
      "Epoch 33/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6635.6846Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 2331.00342\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 7594.3613 - val_loss: 2365.9612 - lr: 1.2500e-04\n",
      "Epoch 33: early stopping\n",
      "Epoch 1/200\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 329103.7188\n",
      "Epoch 1: val_loss improved from inf to 320555.87500, saving model to best_model.h5\n",
      "7/7 [==============================] - 1s 53ms/step - loss: 307240.2188 - val_loss: 320555.8750 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 315026.4062\n",
      "Epoch 2: val_loss improved from 320555.87500 to 319571.87500, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 306603.8750 - val_loss: 319571.8750 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 238169.7812\n",
      "Epoch 3: val_loss improved from 319571.87500 to 317262.68750, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 305194.1562 - val_loss: 317262.6875 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 332811.3125\n",
      "Epoch 4: val_loss improved from 317262.68750 to 311863.25000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 301980.1250 - val_loss: 311863.2500 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 254648.3438\n",
      "Epoch 5: val_loss improved from 311863.25000 to 299968.68750, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 294441.8125 - val_loss: 299968.6875 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 351433.8750\n",
      "Epoch 6: val_loss improved from 299968.68750 to 275817.00000, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 280017.0312 - val_loss: 275817.0000 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 260423.3281\n",
      "Epoch 7: val_loss improved from 275817.00000 to 230729.73438, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 248728.2812 - val_loss: 230729.7344 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 242297.2812\n",
      "Epoch 8: val_loss improved from 230729.73438 to 156952.85938, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 194783.2969 - val_loss: 156952.8594 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 162404.7500\n",
      "Epoch 9: val_loss improved from 156952.85938 to 62729.58203, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 113470.8906 - val_loss: 62729.5820 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 65538.7344\n",
      "Epoch 10: val_loss improved from 62729.58203 to 5044.19775, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 32444.1348 - val_loss: 5044.1978 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11220.6426\n",
      "Epoch 11: val_loss did not improve from 5044.19775\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 15641.7480 - val_loss: 13954.9326 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 28307.2969\n",
      "Epoch 12: val_loss improved from 5044.19775 to 4219.76611, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 21440.0879 - val_loss: 4219.7661 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 15183.4277\n",
      "Epoch 13: val_loss did not improve from 4219.76611\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 8341.0283 - val_loss: 8194.4375 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9915.6221\n",
      "Epoch 14: val_loss did not improve from 4219.76611\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 12334.9043 - val_loss: 8419.0693 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3382.1150\n",
      "Epoch 15: val_loss improved from 4219.76611 to 3766.68457, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 9158.4404 - val_loss: 3766.6846 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6598.7891\n",
      "Epoch 16: val_loss improved from 3766.68457 to 3014.38330, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 8125.5342 - val_loss: 3014.3833 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7726.3433\n",
      "Epoch 17: val_loss improved from 3014.38330 to 2832.08765, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 10195.7256 - val_loss: 2832.0876 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8786.7383\n",
      "Epoch 18: val_loss did not improve from 2832.08765\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 9513.3555 - val_loss: 3628.4331 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9205.3809\n",
      "Epoch 19: val_loss did not improve from 2832.08765\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 9710.7266 - val_loss: 2943.6033 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 11535.2588\n",
      "Epoch 20: val_loss improved from 2832.08765 to 2249.79834, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 11222.7871 - val_loss: 2249.7983 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 10379.6680\n",
      "Epoch 21: val_loss did not improve from 2249.79834\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 7830.5117 - val_loss: 2458.1370 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3999.4492\n",
      "Epoch 22: val_loss improved from 2249.79834 to 2201.45190, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 6908.0552 - val_loss: 2201.4519 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 8225.3271\n",
      "Epoch 23: val_loss improved from 2201.45190 to 1991.39001, saving model to best_model.h5\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 7027.9678 - val_loss: 1991.3900 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7017.3140\n",
      "Epoch 24: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 8373.8779 - val_loss: 2149.2185 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6229.7715\n",
      "Epoch 25: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 8139.6167 - val_loss: 2441.7954 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7514.8242\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 6834.4575 - val_loss: 2456.9971 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 7499.8369\n",
      "Epoch 27: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 8011.4990 - val_loss: 2440.2666 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 3055.7397\n",
      "Epoch 28: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7020.7324 - val_loss: 2312.5068 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9573.3105\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 5695.7344 - val_loss: 2306.8462 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 6336.9580\n",
      "Epoch 30: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 7699.8481 - val_loss: 2330.9604 - lr: 2.5000e-04\n",
      "Epoch 31/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 4653.1079\n",
      "Epoch 31: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 6754.1982 - val_loss: 2286.3201 - lr: 2.5000e-04\n",
      "Epoch 32/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 9230.8145\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 10597.5410 - val_loss: 2517.6589 - lr: 2.5000e-04\n",
      "Epoch 33/200\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 5924.1436Restoring model weights from the end of the best epoch: 23.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1991.39001\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 7417.4302 - val_loss: 2851.6677 - lr: 1.2500e-04\n",
      "Epoch 33: early stopping\n",
      "Validation MSE scores for each fold: [1396.3338623046875, 612.07666015625, 2949.963134765625, 2331.00341796875, 1991.3900146484375]\n",
      "Mean Validation MSE: 1856.15341796875\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_train_scaled and y_train are NumPy arrays\n",
    "X_train_scaled = np.array(X_train_scaled)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "val_scores = []\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, verbose=1)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', verbose=1)\n",
    "    \n",
    "    history = model.fit(X_train_fold, y_train_fold, epochs=200, batch_size=32, \n",
    "                        validation_data=(X_val_fold, y_val_fold), verbose=1, \n",
    "                        callbacks=[lr_scheduler, early_stopping, checkpoint])\n",
    "    \n",
    "    val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "    val_scores.append(val_loss)\n",
    "\n",
    "print(f'Validation MSE scores for each fold: {val_scores}')\n",
    "print(f'Mean Validation MSE: {np.mean(val_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best weights\n",
    "model.load_weights('best_model.h5')\n",
    "\n",
    "# Assuming you have X_test_scaled and y_test as your test dataset\n",
    "# Replace these with your actual test datasets\n",
    "X_test_scaled = np.array(X_test_scaled)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict the prices on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Plot Predicted vs Actual Prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')  # Diagonal line\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Predicted vs Actual Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('trained_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
